from langchain_community.document_loaders import WikipediaLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain import PromptTemplate

llm  = ChatGroq(
    model_name="deepseek-r1-distill-qwen-32b",
    temperature=0.0
)


reader_template = """As a Question answering assitant, generate an answer to the input question using the context provided.
Follow the below guidelines while answering the question.
- Use the context to answer the question. Do not answer out of the context available.
- Be concise and clear in your language.
- If you do not know the answer just say you - "Sorry, I do not know this!"
Use the context: {context} for the question: {question} to generate the answer.
Helpful Answer:"""
  
embeddings = OllamaEmbeddings(
    model="mxbai-embed-large",
)

def load_wikipedia(web_path) :
    """
    Load the contents of a wikipedia page given a web path or title.

    Parameters
    ----------
    web_path : str
        The path to the wikipedia page. This can either be a full URL or just the title of the page.

    Returns
    -------
    list of langchain.DocumentPage
        A list of DocumentPage objects, each containing the text of a portion of the wikipedia page.
    """
    if web_path.startswith("https"):
        search = web_path.split("/")[-1]
    else:
        search = web_path
    # Load and chunk contents of the blog
    loader = WikipediaLoader(
        search,
        load_max_docs=1
    )
    
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    all_splits = text_splitter.split_documents(docs)
    print(f"Loaded {len(all_splits)} documents")
    return all_splits


def store_wikipedia(all_splits):

    # Create a FAISS index from the given splits and embeddings
    db = FAISS.from_documents(all_splits, embedding=embeddings)
    print("Done Embedding")
    return db


def qa_reader(ques ,db):
    retriever = db.as_retriever()
    
    #1. Generate the prompt using prompt template
    reader_prompt = PromptTemplate(template=reader_template, input_variables=["context", "question"])
    
    #2. Use LLM chain to create llm instance with the llm model and the prompt
    llm_chain = (
        {"context" : RunnablePassthrough() , "question": RunnablePassthrough()} 
        | reader_prompt 
        | llm)
    
    #3. Retrieve relevant documents from the retriever for the input query
    docs= retriever.get_relevant_documents(ques)
    
    #4. Pass the retrieved documents as the context and the input query to the LLM Chain created in step 2
    result = llm_chain.invoke({"context": docs, "question": ques})
    
    #5. Return the output generated by the LLM
    return result.content